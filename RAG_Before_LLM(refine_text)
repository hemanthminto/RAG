
import fitz, pdfplumber, pytesseract, io, os, re, requests
from PIL import Image
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# Load models
embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
generator = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
captioner = pipeline("image-to-text", model="Salesforce/blip-image-captioning-base")

# Similarity thresholds by content type
THRESHOLDS = {
    "text": 0.50,
    "table": 0.50,
    "figure": 0.50,
    "formula": 0.50,
    "web": 0.30
}

# Global counts
doc_stats = {
    "text_chunks": 0,
    "table_chunks": 0,
    "num_tables": 0,
    "figure_chunks": 0,
    "num_images": 0,
    "formula_chunks": 0,
    "num_formulas": 0,
    "web_chunks": 0,
    "num_urls": 0
}

# Extract content from PDF
def extract_pdf_data(pdf_path):
    chunks = []
    seen_urls = set()
    doc = fitz.open(pdf_path)
    plumber = pdfplumber.open(pdf_path)
    for i, page in enumerate(doc):
        text = page.get_text()
        if text:
            for para in text.split("\n\n"):
                if para.strip():
                    chunks.append({"type": "text", "source": f"page {i+1}", "text": para.strip()})
                    doc_stats["text_chunks"] += 1
        # Tables
        tables = plumber.pages[i].extract_tables()
        for table in tables:
            if table:
                try:
                    df = pd.DataFrame(table[1:], columns=table[0])
                    md = df.to_markdown(index=False)
                    chunks.append({"type": "table", "source": f"page {i+1}", "text": md})
                    doc_stats["table_chunks"] += 1
                    doc_stats["num_tables"] += 1
                except:
                    continue
        # Images
        for img in page.get_images(full=True):
            doc_stats["num_images"] += 1
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image = Image.open(io.BytesIO(image_bytes))
            ocr_text = pytesseract.image_to_string(image).strip()
            try:
                caption = captioner(image)[0]['generated_text']
            except:
                caption = ""
            combined = f"OCR-Caption: {ocr_text}\n: {caption}".strip()
            if combined:
                chunks.append({"type": "figure", "source": f"page {i+1}", "text": combined})
                doc_stats["figure_chunks"] += 1
        # Formulas
        if text:
            for line in text.splitlines():
                if re.search(r"[=<>+\-√ó√∑‚àö^]", line) and len(line) < 120:
                    chunks.append({"type": "formula", "source": f"page {i+1}", "text": line.strip()})
                    doc_stats["formula_chunks"] += 1
                    doc_stats["num_formulas"] += 1
        # URLs
        urls = re.findall(r"https?://\S+|www\.\S+", text)
        for url in urls:
            url = url.strip(".,)")
            if url in seen_urls:
                continue
            seen_urls.add(url)
            try:
                r = requests.get(url, timeout=5)
                if r.ok and 'text/html' in r.headers.get('Content-Type', ''):
                    soup = BeautifulSoup(r.text, 'html.parser')
                    content = soup.get_text(separator=" ").strip()
                    if content:
                        chunks.append({"type": "web", "source": url, "text": content[:1000]})
                        doc_stats["web_chunks"] += 1
                        doc_stats["num_urls"] += 1
            except:
                continue
    doc.close()
    plumber.close()
    return chunks

# Build vector index
def build_index(chunks):
    index = []
    for c in chunks:
        text = c.get("text", "").strip()
        if text:
            emb = embedder.encode(text, convert_to_numpy=True)
            index.append({"chunk": c, "emb": emb})
    return index

# Search with cosine similarity and thresholds
def search_chunks(query, index, k_text=3):
    q_emb = embedder.encode([query])[0]
    results = []
    text_matches = []
    for item in index:
        chunk = item["chunk"]
        sim = np.dot(q_emb, item["emb"]) / (np.linalg.norm(q_emb) * np.linalg.norm(item["emb"]))
        ctype = chunk.get("type", "text")
        chunk["score"] = round(sim, 3)
        if ctype == "text":
            text_matches.append((sim, chunk))
        elif sim >= THRESHOLDS.get(ctype, 1.0):
            results.append((sim, chunk))
    text_matches.sort(key=lambda x: x[0], reverse=True)
    results.sort(key=lambda x: x[0], reverse=True)
    results.extend(text_matches[:k_text])
    return [r[1] for r in results]

# Generate answer and show matching scores
def generate_answer(query, chunks):
    context = ""
    appendix = {"table": [], "figure": [], "formula": [], "web": [], "text": []}
    sim_summary = {"table": 0, "figure": 0, "formula": 0, "web": 0, "text": 0}
    retrieved_counts = {"table": 0, "figure": 0, "formula": 0, "web": 0, "text": 0}

    for c in chunks:
        c_type = c.get("type", "text")
        c_text = c.get("text", "").strip()
        if not c_text:
            continue
        score = c.get("score", 0)
        sim_summary[c_type] = max(sim_summary.get(c_type, 0), score)
        context += f"{c_type.upper()} ({c.get('source')}): {c_text}\n"
        appendix[c_type].append(c_text)
        retrieved_counts[c_type] += 1

    prompt = f"You are a science tutor. Use the context to explain and answer.\n\nContext:\n{context}\n\nQuestion: {query}\nAnswer:"
    tokens = tokenizer(prompt, return_tensors="pt", truncation=True)
    out = generator.generate(**tokens, max_length=512, num_beams=4)
    answer = tokenizer.decode(out[0], skip_special_tokens=True)

    for label in ["table", "figure", "formula", "web"]:
        if appendix[label]:
            answer += f"\n\nüìå Relevant {label.capitalize()}:\n" + "\n\n".join(appendix[label][:2])
    return answer, chunks, sim_summary, retrieved_counts

# Main program loop
if __name__ == "__main__":
    import sys
    pdf_path = input("üìÑ Enter path to your PDF file:\n> ").strip()
    if not os.path.exists(pdf_path):
        print("‚ùå File not found. Please check the path and try again.")
        sys.exit(1)

    print(f"üîÑ Initializing for: {pdf_path}")
    chunks = extract_pdf_data(pdf_path)
    index = build_index(chunks)
    print(f"‚úÖ Loaded and indexed {len(chunks)} total chunks.\n")

    # Show global document stats
    print("üìä Document Summary:")
    print(f" - Text chunks     : {doc_stats['text_chunks']}")
    print(f" - Tables found    : {doc_stats['num_tables']}")
    print(f" - Table chunks    : {doc_stats['table_chunks']}")
    print(f" - Formulas found  : {doc_stats['num_formulas']}")
    print(f" - Formula chunks  : {doc_stats['formula_chunks']}")
    print(f" - Images found    : {doc_stats['num_images']}")
    print(f" - Figure chunks   : {doc_stats['figure_chunks']}")
    print(f" - URLs found      : {doc_stats['num_urls']}")
    print(f" - Web chunks      : {doc_stats['web_chunks']}")

    while True:
        user_query = input("\nüß† Ask a question (or type 'exit' to quit):\n> ")
        if user_query.lower() in {"exit", "quit"}:
            print("üëã Exiting. Bye...!!")
            break
        print("üîç Searching for relevant info...")
        top_chunks = search_chunks(user_query, index)
        print("üß† Generating answer...\n")
        answer, scored_chunks, sim_summary, retrieved_counts = generate_answer(user_query, top_chunks)

        # Show match percentages
        print("\nüìä Match Percentages (max similarity):")
        for t in ["text", "table", "figure", "formula", "web"]:
            score = sim_summary.get(t, 0)
            print(f" - {t.capitalize():<8}: {round(score * 100, 1)}%")

        # Show retrieved counts
        print("\nüì¶ Retrieved Chunks by Type:")
        for t in ["text", "table", "figure", "formula", "web"]:
            print(f" - {t.capitalize():<8}: {retrieved_counts.get(t, 0)}")

        print("\nüìö Answer:\n")
        print(answer)
